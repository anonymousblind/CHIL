{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, ConcatDataset, IterableDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQ_LEN=20\n",
    "#D_HID_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cuda(tensor):\n",
    "    if T.cuda.is_available():\n",
    "        tensor = tensor.cuda(0)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Embedding(nn.Module):\n",
    "    def __init__ (self, inputDimSize, embSize):\n",
    "        super(Custom_Embedding, self).__init__()\n",
    "        self.inputDimSize = inputDimSize\n",
    "        self.embSize = embSize\n",
    "        \n",
    "        self.W_emb = nn.Parameter(torch.randn(self.inputDimSize, self.embSize) * 0.01)\n",
    "        self.b_emb = nn.Parameter(torch.zeros(self.embSize) * 0.01) \n",
    "       \n",
    "    def forward(self, x):\n",
    "        x=x.cuda(0)\n",
    "        return torch.tanh(x@self.W_emb + self.b_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' C-RNN-GAN generator\n",
    "    '''\n",
    "    def __init__(self, input_dim, drug_dim, age_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # params\n",
    "        self.visitEmbedding = Custom_Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.drugEmbedding = Custom_Embedding(drug_dim, emb_dim)\n",
    "        \n",
    "        self.ageEmbedding = nn.Embedding(age_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, src, drug, age):\n",
    "        ''' Forward prop\n",
    "        '''\n",
    "        #src=src.cuda(0)\n",
    "        #age=age.cuda(0)\n",
    "        #print(\"==============Inside Encoder=============\")\n",
    "        #src = [batch_size, seq_len, num_feats]\n",
    "        batch_size=src.shape[0]\n",
    "        seq_len=src.shape[1]\n",
    "        #print(\"src\",src.shape)\n",
    "        src = src.view(-1, src.size(2)) # (N*seq_len, num_feats)\n",
    "        #print(\"src\",src.shape)\n",
    "        #src=src.double()\n",
    "        visitEmbedded = self.visitEmbedding(src)\n",
    "        #print(\"visitEmbedded\",visitEmbedded.shape)\n",
    "        visitEmbedded = visitEmbedded.view(batch_size,seq_len, -1)\n",
    "#         #print(\"visitEmbedded\",visitEmbedded[0:2,:])\n",
    "#         visitEmbedded = visitEmbedded.view(*src_size, -1)\n",
    "        #print(\"visitEmbedded\",visitEmbedded.shape)\n",
    "#         #print(\"listEmbedding\",list(self.visitEmbedding.parameters()))\n",
    "#         #print(\"visitEmbedded\",visitEmbedded[0,0,:,:])\n",
    "#         #print(\"visitEmbedded\",visitEmbedded[0,0,0,:])\n",
    "#         visitEmbedded = visitEmbedded.sum(2)\n",
    "#         #print(\"visitEmbedded\",visitEmbedded.shape)\n",
    "#         #print(\"visitEmbedded\",visitEmbedded[0,0,:])\n",
    "\n",
    "############DRUG EMBEDDING###################        \n",
    "        batch_size=drug.shape[0]\n",
    "        seq_len=drug.shape[1]\n",
    "        drug = drug.view(-1, drug.size(2)) # (N*seq_len, num_feats)\n",
    "        #print(\"drug\",drug.shape)\n",
    "        drugEmbedded = self.drugEmbedding(drug)\n",
    "        drugEmbedded = drugEmbedded.view(batch_size,seq_len, -1)\n",
    "        \n",
    "        \n",
    "        age_size=age.size()\n",
    "        #print(\"age\",age.shape)\n",
    "        age = age.long()\n",
    "#         #print(\"age\",age[0,:])\n",
    "#         #print(\"listEmbedding\",list(self.ageEmbedding.parameters()))\n",
    "#         age = age.view(-1, age.size(2)) # (N*seq_len, num_feats)\n",
    "#         age=age.squeeze()\n",
    "#         #print(\"age\",age.shape)\n",
    "        ageEmbedded = self.ageEmbedding(age)\n",
    "        #print(\"ageEmbedded\",ageEmbedded.shape)\n",
    "        #ageEmbedded = ageEmbedded.view(*age_size, -1)\n",
    "        ##print(\"ageEmbedded\",ageEmbedded.shape)\n",
    "        #ageEmbedded = ageEmbedded.sum(2)\n",
    "        ##print(\"ageEmbedded\",ageEmbedded.shape)\n",
    "        \n",
    "        embedded = visitEmbedded + ageEmbedded + drugEmbedded \n",
    "        #print(\"embedded\",embedded.shape)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #print(\"embedded\",embedded.shape)\n",
    "        \n",
    "        #embedded = [batch_size, seq_len, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #print(\"hidden\",hidden.shape)\n",
    "        #print(\"outputs\",outputs.shape)        \n",
    "        #outputs = [batch_size, seq_len, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        ##print(\"hidden\",hidden[-2,:,:].shape)\n",
    "        ##print(\"hidden\",hidden[-1,:,:].shape)\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]. \n",
    "        #hidden is last visit representation\n",
    "        #outputs are always from the last layer. outputs are given at each GRU cell\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        #print(\"hidden\",hidden.shape)\n",
    "        #hidden=hidden.cuda(0)\n",
    "        hidden = self.fc(hidden)\n",
    "        #print(\"hidden\",hidden.shape)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        #print(\"hidden\",hidden.shape)\n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        #print(\"=====================inside attention======================\")\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #print(\"hidden\",hidden.shape)\n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        #print(\"energy\",energy.shape)\n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        #print(\"attention\",attention.shape)\n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, age_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.visitEmbedding = Custom_Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.ageEmbedding = nn.Embedding(age_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, age, hidden, encoder_outputs):\n",
    "             \n",
    "        #print(\"================Inside Decoder===============\")\n",
    "        #print(\"input\",input.shape)\n",
    "        #print(\"age\",age.shape)\n",
    "        #print(\"hidden\",hidden.shape)\n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        #input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        batch_size=input.shape[0]\n",
    "        input=input.float()\n",
    "        visitEmbedded = self.visitEmbedding(input)\n",
    "        #print(\"visitEmbedded\",visitEmbedded.shape)\n",
    "        \n",
    "        age = age.long()\n",
    "        ageEmbedded = self.ageEmbedding(age)\n",
    "        #print(\"ageEmbedded\",ageEmbedded.shape)\n",
    "        \n",
    "        embedded = visitEmbedded + ageEmbedded\n",
    "        embedded = self.dropout(embedded)\n",
    "        embedded=embedded.unsqueeze(0)\n",
    "        #print(\"embedded\",embedded.shape)\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)        \n",
    "        #print(\"out of attention\")\n",
    "        #a = [batch size, src len]\n",
    "        #print(\"a\",a.shape)\n",
    "        a = a.unsqueeze(1)\n",
    "        #print(\"a\",a.shape)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        #encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        #print(\"weighted\",weighted.shape)\n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        #weighted=weighted.squeeze()\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #print(\"weighted\",weighted.shape)\n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        #print(\"embedded\",embedded.shape)\n",
    "        #print(\"hidden\",hidden.shape)    \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #print(\"rnn_input\",rnn_input.shape)\n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        #print(\"output\",output.shape) \n",
    "        #print(\"hidden\",hidden.shape) \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        #print(\"embedded\",embedded.shape)\n",
    "        #print(\"output\",output.shape) \n",
    "        #print(\"weighted\",weighted.shape)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #print(\"prediction\",prediction .shape)\n",
    "        ##print(\"prediction\",prediction)\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)#,a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, drug, trg, age, mask, train):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [trg len, batch size, trg_features]\n",
    "        trg=trg.permute(1,0,2)\n",
    "        #print(\"trg\",trg.shape)\n",
    "        #print(\"===========Inside seq2seq===========\")\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_features = self.decoder.output_dim\n",
    "        \n",
    "        #print(\"batch_size\",batch_size)\n",
    "        #print(\"trg_len\",trg_len)\n",
    "        #print(\"trg_features\",trg_features)\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_features).cuda(0)\n",
    "        attn = torch.zeros(trg_len, batch_size, trg_len).cuda(0)\n",
    "        #print(\"outputs\",outputs.shape)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src, drug, age)\n",
    "        #print(\"===========Outside Encoder===========\")\n",
    "        \n",
    "        #print(\"encoder_outputs\",encoder_outputs.shape)\n",
    "        #print(\"hidden\",hidden.shape)\n",
    "        \n",
    "        #print(\"===========Preparing input for Decoder===========\")\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = torch.zeros(batch_size, trg_features, dtype=T.long)#trg[0,:]\n",
    "        #print(\"input\",input.shape)\n",
    "        \n",
    "        age=age.permute(1,0)\n",
    "        #print(\"age\",age.shape)\n",
    "        #inputAge = age[0]#torch.zeros(batch_size, dtype=T.long)\n",
    "        \n",
    "        mask=mask.permute(1,0)\n",
    "        \n",
    "        for t in range(0, trg_len):\n",
    "            #print(\"=================Inside for loop================\")\n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            inputAge=age[t]\n",
    "            #print(\"age\",inputAge)\n",
    "            output, hidden = self.decoder(input, inputAge, hidden, encoder_outputs)\n",
    "            #print(\"==============Outside decoder=================\")\n",
    "            #print(\"output\",output.shape)\n",
    "            #print(\"hidden\",hidden.shape)\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            #attn[t] = a\n",
    "            #print(\"OUTPUTS\",outputs.shape)\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            #print(\"top1\",top1.shape)\n",
    "            ##print(\"top1\",top1[0:2])\n",
    "            \n",
    "            ##print(\"age\",inputAge)\n",
    "            #print(\"======Deciding next input for decoder=========\")\n",
    "            prediction = F.one_hot(top1, num_classes=trg_features)\n",
    "            #print(\"prediction\",prediction.shape)\n",
    "            ##print(\"prediction\",prediction[0:2,:])\n",
    "            #print(\"mask\",mask.shape)\n",
    "            ##print(\"mask\",mask[t])\n",
    "            ##print(\"mask\",mask[t].unsqueeze(1).repeat(1,trg_features))\n",
    "            inputMask=mask[t]\n",
    "            #print(\"inputMask\",inputMask.shape)\n",
    "            #inputMask[1]=1\n",
    "            inputMask=inputMask.unsqueeze(1).repeat(1,trg_features)\n",
    "            #print(\"inputMask\",inputMask.shape)\n",
    "            #print(\"trg\",trg[t].shape)\n",
    "            ##print(\"inputMask\",inputMask[0:2,:])\n",
    "            inputMask=inputMask.float()\n",
    "            trg[t]=trg[t].float()\n",
    "            prediction=prediction.float()\n",
    "            context=trg[t] * inputMask + prediction * (1-inputMask)\n",
    "            #print(\"context\",context.shape)\n",
    "            \n",
    "            ##print(\"inputMask\",inputMask[0:2,:])\n",
    "            ##print(\"trg\",trg[t,0:2,:])\n",
    "            ##print(\"prediction\",prediction[0:2,:])\n",
    "            ##print(\"context\",context[0:2,:])\n",
    "\n",
    "            if train:\n",
    "                #print(\"=========Training===========\")\n",
    "                input = context\n",
    "            else:\n",
    "                #print(\"===========Testing==========\")\n",
    "                input = prediction\n",
    "            #print(\"Next input for decoder\",input.shape)\n",
    "\n",
    "        return outputs#, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,MAX_SEQ_LEN, D_HID_SIZE):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.D_HID_SIZE=D_HID_SIZE\n",
    "        self.SEQ_LEN = MAX_SEQ_LEN\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.rnn_cell = nn.LSTMCell(1, self.D_HID_SIZE)\n",
    "        self.regression1 = nn.Linear(self.D_HID_SIZE, 5)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.regression2 = nn.Linear(5, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def merge_score(self, score_f):\n",
    "        \n",
    "        #print(\"Foward Scores\",score_f['scoresSig'][0,:])\n",
    "        #print(\"Backward Scores\",score_b['scores'][0,:])\n",
    "        #print(\"Missing\",score_f['missing'][0,:])\n",
    "        #print(\"Foward Scores\",score_f['scores'].size())\n",
    "        #print(\"Backward Scores\",score_b['scores'].size())\n",
    "        #print(\"Missing\",score_f['missing'].size())\n",
    "        \n",
    "        #Calculate Loss for Sigmid layer\n",
    "        Tensor = torch.cuda.FloatTensor\n",
    "        \n",
    "        score_f['scoresSig'] = torch.flatten(score_f['scoresSig'])\n",
    "        score_f['missing'] = torch.flatten(score_f['missing'])\n",
    "        \n",
    "        \n",
    "        real_ids = (score_f['missing'].nonzero())\n",
    "        fake_ids = ((1-score_f['missing']).nonzero())\n",
    "        \n",
    "        # Loss function\n",
    "        adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable((score_f['missing'])[real_ids], requires_grad=False)\n",
    "        fake = Variable((score_f['missing'])[fake_ids], requires_grad=False)\n",
    "        validG = Variable((1-score_f['missing'])[fake_ids], requires_grad=False)\n",
    "        \n",
    "        #print(\"Valid\",valid.size())\n",
    "        #print(\"fake\",fake.size())\n",
    "               \n",
    "        #ret_b['scores'] = ret_b['scores'] * ret_b['missing']\n",
    "        #print(\"Final Scores\",ret_b['imputations'][0,:])\n",
    "                \n",
    "        if(fake_ids.size()[0]==0 ):     \n",
    "            loss_gSig=Variable(torch.cuda.FloatTensor([0]), requires_grad=True)\n",
    "        else:\n",
    "            loss_gF = adversarial_loss((score_f['scoresSig'])[fake_ids], validG)\n",
    "            loss_gSig=loss_gF\n",
    "        \n",
    "        loss_dReal = adversarial_loss((score_f['scoresSig'])[real_ids], valid)\n",
    "        if(fake_ids.size()[0]==0 ): \n",
    "            loss_dSig = loss_dReal\n",
    "        else:\n",
    "            loss_dFake = adversarial_loss((score_f['scoresSig'])[fake_ids], fake)\n",
    "            loss_dSig = (loss_dReal + loss_dFake)/2\n",
    "        #print(loss_dSig,loss_gSig)\n",
    "        return {'loss_d': loss_dSig , 'loss_g': loss_gSig}\n",
    "        \n",
    "    def forward(self, values, masks, direct):\n",
    "        \n",
    "        h = Variable(torch.zeros((values.size()[0], self.D_HID_SIZE)))\n",
    "        c = Variable(torch.zeros((values.size()[0], self.D_HID_SIZE)))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h, c = h.cuda(0), c.cuda(0)\n",
    "            values, masks = values.cuda(0), masks.cuda(0)\n",
    "            \n",
    "        scoresSig=[]\n",
    "        missing=[]\n",
    "        if(direct==\"forward\"):\n",
    "\n",
    "            for t in range(self.SEQ_LEN):\n",
    "                #print(\"===============\",t,\"======================\")\n",
    "                x = values[:, t]\n",
    "                x=x.unsqueeze(dim=1)\n",
    "                m = masks[:, t]\n",
    "                #print(\"Input\",x.size())\n",
    "                #print(\"Input\",x[0])\n",
    "\n",
    "                x_h = self.regression1(h)\n",
    "                x_h = self.leaky(x_h)\n",
    "                x_h = self.regression2(x_h)\n",
    "                x_h2 = self.sig(x_h)\n",
    "                #print(\"Output regression\",x_h.size())\n",
    "                #print(\"Output regression\",x_h[0])\n",
    "                #print(\"Discriminator output\",x_h2.size())\n",
    "                #print(\"Discriminator output\",x_h2[0])\n",
    "\n",
    "                #print(\"Mask\",m.size())\n",
    "\n",
    "                m=m.unsqueeze(dim=1)\n",
    "                \n",
    "                #print(\"i am here\")\n",
    "\n",
    "                h, c = self.rnn_cell(x, (h, c))\n",
    "                #print(\"i am here\")\n",
    "\n",
    "                #imputations.append(x_c[:,316].unsqueeze(dim = 1))\n",
    "                scoresSig.append(x_h2[:,0].unsqueeze(dim = 1))\n",
    "                #print(\"i am here\")\n",
    "                missing.append(m)\n",
    "                #print(\"i am here\")\n",
    "                #print(\"to be appended\",m.size())\n",
    "                #print(\"Imputations\",len(imputations))\n",
    "                #print(\"Scores\",scores[0].size())\n",
    "        \n",
    "        elif(direct==\"backward\"):\n",
    "\n",
    "            for t in range(self.SEQ_LEN-1,-1,-1):\n",
    "                #print(\"===============\",t,\"======================\")\n",
    "                x = values[:, t]\n",
    "                x=x.unsqueeze(dim=1)\n",
    "                m = masks[:, t]\n",
    "                #print(\"Input\",x.size())\n",
    "                #print(\"Input\",x[0])\n",
    "\n",
    "                x_h = self.regression1(h)\n",
    "                x_h = self.leaky(x_h)\n",
    "                x_h = self.regression2(x_h)\n",
    "                x_h2 = self.sig(x_h)\n",
    "                #print(\"Discriminator output\",x_h.shape)\n",
    "\n",
    "                #print(\"Output regression\",x_h.size())\n",
    "                #print(\"Mask\",m.size())\n",
    "\n",
    "                m=m.unsqueeze(dim=1)\n",
    "                \n",
    "                #print(\"d\",d[:,0].unsqueeze(dim=1).size())\n",
    "\n",
    "                h, c = self.rnn_cell(x, (h, c))\n",
    "\n",
    "                #imputations.append(x_c[:,316].unsqueeze(dim = 1))\n",
    "                scoresSig.append(x_h2[:,0].unsqueeze(dim = 1))\n",
    "                missing.append(m)\n",
    "                #print(\"to be appended\",m.size())\n",
    "                #print(\"Imputations\",len(imputations))\n",
    "                #print(\"Scores\",scores[0].size())\n",
    "        \n",
    "        scoresSig = torch.cat(scoresSig, dim = 1)\n",
    "        missing = torch.cat(missing, dim = 1)\n",
    "        #print(\"Scores\",len(scoresSig),scoresSig[0].size())\n",
    "        return self.merge_score({'scoresSig': scoresSig, 'missing':missing})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, path, chunksize,length,seq_len,flag):\n",
    "        self.path = path\n",
    "        self.chunksize = chunksize\n",
    "        self.len = int(length)#number of times total getitem is called\n",
    "        self.seq_len=seq_len\n",
    "        self.flag=flag\n",
    "        self.reader=pd.read_csv(\n",
    "                self.path,header=0,\n",
    "                chunksize=self.chunksize)#,names=['data']))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.reader.get_chunk(self.chunksize)\n",
    "        #sex=pd.read_csv('C:\\\\Users/mehak/Desktop/demo.csv',header=0)\n",
    "        #sex=sex[['person_id','Sex']]\n",
    "        #data = pd.merge(data, sex, how='left', on=['person_id'])\n",
    "        #print(data.shape)\n",
    "        #data=data.sort_values(by=['RANDOM_PATIENT_ID','VISIT_YEAR','VISIT_MONTH'])\n",
    "        #print(data['RANDOM_PATIENT_ID'].unique())\n",
    "#         del data['person_id']\n",
    "#         print(data.columns.get_loc('BMI'))\n",
    "        #print(data.columns)\n",
    "\n",
    "        data=data.replace(np.inf,0)\n",
    "        data=data.replace(np.nan,0)\n",
    "        \n",
    "        if(self.flag==0):\n",
    "            data['Rhinitis allergic']=0\n",
    "            data=data.rename(columns={'age':'Age'})\n",
    "            data['Age']=data['Age'].apply(lambda x: ((x*12)/3)-1)\n",
    "            pids=data['person_id']\n",
    "            pids = T.as_tensor(pids.values.astype(float), dtype=T.long)\n",
    "#             print(\"pids\",pids.shape)\n",
    "#             print(\"pids\",list(pids))\n",
    "#             print(\"========================================================\")\n",
    "\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "    #         print(list(data[:,0]))\n",
    "    #         print(\"========================================================\")\n",
    "            #data=T.from_numpy(data)\n",
    "            #data=data.double()\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            #print(data.shape)\n",
    "            #print(\"age\",data[0,:,203])\n",
    "            #mask=pd.DataFrame()\n",
    "            #mask = data.loc[data['LABS_LDL_MEAN']>0,'LABS_LDL_MEAN']=1\n",
    "            #df[df['LABS_LDL_MEAN']<0].count()\n",
    "            return data,pids\n",
    "        elif(self.flag==1):\n",
    "            #print(data.columns)\n",
    "            #data['DRUG_1550720']=0\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            return data\n",
    "        elif(self.flag==2):\n",
    "            data = T.as_tensor(data.values.astype(float), dtype=T.float32)\n",
    "            data=data.view(int(data.shape[0]/self.seq_len), self.seq_len, data.shape[1])\n",
    "            \n",
    "            return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, args, patience=7, verbose=False, delta=-0.01):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf#11.1179\n",
    "        self.delta = delta\n",
    "        self.args=args\n",
    "\n",
    "    def __call__(self, val_loss, model, optimizer, save_path):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, save_path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            if score > self.best_score + 0:\n",
    "                self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, save_path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, optimizer, save_path):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        if  self.args.discriminator:\n",
    "            T.save({\n",
    "                \"G_model\": model['g'].state_dict(),\n",
    "                \"D_model\": model['d'].state_dict(),\n",
    "                'G_trainer': optimizer['g'].state_dict(),\n",
    "                'D_trainer': optimizer['d'].state_dict()\n",
    "            }, save_path)\n",
    "        else:\n",
    "            T.save({\n",
    "                \"G_model\": model['g'].state_dict(),\n",
    "                'G_trainer': optimizer['g'].state_dict()\n",
    "            }, save_path)\n",
    "        self.val_loss_min = -self.best_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
